{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BSOityemqnx9"
      },
      "outputs": [],
      "source": [
        "!rm -r -f /content/sample_data/Results\n",
        "!rm -r -f /content/sample_data/REFS\n",
        "!rm -r -f /content/sample_data/TARGET\n",
        "\n",
        "!mkdir  /content/sample_data/REFS\n",
        "!mkdir  /content/sample_data/Results\n",
        "!mkdir  /content/sample_data/TARGET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSLJyH4IdAD3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "sx_Uja5LldYh",
        "outputId": "aa57562a-d69f-4988-b707-effffda859f5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c1d12cea0e09>\u001b[0m in \u001b[0;36m<cell line: 366>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;31m# Load the input video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0mvideo_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0mvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Make a reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpix_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rgba\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_mask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb24\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         self.reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt,\n\u001b[0m\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0m\u001b[1;32m     36\u001b[0m                                    fps_source)\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"No such file or directory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[0m\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                       \"path.\")%filename)\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file /content/sample_data/test2.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        }
      ],
      "source": [
        "#@title\n",
        "import moviepy.video.fx.all as vfx\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage import color\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from IPython import embed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from IPython import embed\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "import moviepy.video.fx.all as vfx\n",
        "def load_img(img_path):\n",
        "\tout_np = np.asarray(Image.open(img_path))\n",
        "\tif(out_np.ndim==2):\n",
        "\t\tout_np = np.tile(out_np[:,:,None],3)\n",
        "\treturn out_np\n",
        "\n",
        "def resize_img(img, HW=(256,256), resample=3):\n",
        "\treturn np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n",
        "\n",
        "def preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\n",
        "\t# return original size L and resized L as torch Tensors\n",
        "\timg_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
        "\n",
        "\timg_lab_orig = color.rgb2lab(img_rgb_orig)\n",
        "\timg_lab_rs = color.rgb2lab(img_rgb_rs)\n",
        "\n",
        "\timg_l_orig = img_lab_orig[:,:,0]\n",
        "\timg_l_rs = img_lab_rs[:,:,0]\n",
        "\n",
        "\ttens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n",
        "\ttens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n",
        "\n",
        "\treturn (tens_orig_l, tens_rs_l)\n",
        "\n",
        "def postprocess_tens(tens_orig_l, out_ab, mode='bilinear'):\n",
        "\t# tens_orig_l \t1 x 1 x H_orig x W_orig\n",
        "\t# out_ab \t\t1 x 2 x H x W\n",
        "\n",
        "\tHW_orig = tens_orig_l.shape[2:]\n",
        "\tHW = out_ab.shape[2:]\n",
        "\n",
        "\t# call resize function if needed\n",
        "\tif(HW_orig[0]!=HW[0] or HW_orig[1]!=HW[1]):\n",
        "\t\tout_ab_orig = F.interpolate(out_ab, size=HW_orig, mode='bilinear')\n",
        "\telse:\n",
        "\t\tout_ab_orig = out_ab\n",
        "\n",
        "\tout_lab_orig = torch.cat((tens_orig_l, out_ab_orig), dim=1)\n",
        "\treturn color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class BaseColor(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(BaseColor, self).__init__()\n",
        "\n",
        "\t\tself.l_cent = 50.\n",
        "\t\tself.l_norm = 100.\n",
        "\t\tself.ab_norm = 110.\n",
        "\n",
        "\tdef normalize_l(self, in_l):\n",
        "\t\treturn (in_l-self.l_cent)/self.l_norm\n",
        "\n",
        "\tdef unnormalize_l(self, in_l):\n",
        "\t\treturn in_l*self.l_norm + self.l_cent\n",
        "\n",
        "\tdef normalize_ab(self, in_ab):\n",
        "\t\treturn in_ab/self.ab_norm\n",
        "\n",
        "\tdef unnormalize_ab(self, in_ab):\n",
        "\t\treturn in_ab*self.ab_norm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ECCVGenerator(BaseColor):\n",
        "    def __init__(self, norm_layer=nn.BatchNorm2d):\n",
        "        super(ECCVGenerator, self).__init__()\n",
        "\n",
        "        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model1+=[nn.ReLU(True),]\n",
        "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n",
        "        model1+=[nn.ReLU(True),]\n",
        "        model1+=[norm_layer(64),]\n",
        "\n",
        "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model2+=[nn.ReLU(True),]\n",
        "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n",
        "        model2+=[nn.ReLU(True),]\n",
        "        model2+=[norm_layer(128),]\n",
        "\n",
        "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[norm_layer(256),]\n",
        "\n",
        "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[norm_layer(512),]\n",
        "\n",
        "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[norm_layer(512),]\n",
        "\n",
        "        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[norm_layer(512),]\n",
        "\n",
        "        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[norm_layer(512),]\n",
        "\n",
        "        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n",
        "        model8+=[nn.ReLU(True),]\n",
        "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model8+=[nn.ReLU(True),]\n",
        "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model8+=[nn.ReLU(True),]\n",
        "\n",
        "        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n",
        "\n",
        "        self.model1 = nn.Sequential(*model1)\n",
        "        self.model2 = nn.Sequential(*model2)\n",
        "        self.model3 = nn.Sequential(*model3)\n",
        "        self.model4 = nn.Sequential(*model4)\n",
        "        self.model5 = nn.Sequential(*model5)\n",
        "        self.model6 = nn.Sequential(*model6)\n",
        "        self.model7 = nn.Sequential(*model7)\n",
        "        self.model8 = nn.Sequential(*model8)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n",
        "        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
        "\n",
        "    def forward(self, input_l):\n",
        "        conv1_2 = self.model1(self.normalize_l(input_l))\n",
        "        conv2_2 = self.model2(conv1_2)\n",
        "        conv3_3 = self.model3(conv2_2)\n",
        "        conv4_3 = self.model4(conv3_3)\n",
        "        conv5_3 = self.model5(conv4_3)\n",
        "        conv6_3 = self.model6(conv5_3)\n",
        "        conv7_3 = self.model7(conv6_3)\n",
        "        conv8_3 = self.model8(conv7_3)\n",
        "        out_reg = self.model_out(self.softmax(conv8_3))\n",
        "\n",
        "        return self.unnormalize_ab(self.upsample4(out_reg))\n",
        "\n",
        "def eccv16(pretrained=True):\n",
        "\tmodel = ECCVGenerator()\n",
        "\tif(pretrained):\n",
        "\t\timport torch.utils.model_zoo as model_zoo\n",
        "\t\tmodel.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth',map_location='cpu',check_hash=True))\n",
        "\treturn model\n",
        "\n",
        "class SIGGRAPHGenerator(BaseColor):\n",
        "    def __init__(self, norm_layer=nn.BatchNorm2d, classes=529):\n",
        "        super(SIGGRAPHGenerator, self).__init__()\n",
        "\n",
        "        # Conv1\n",
        "        model1=[nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model1+=[nn.ReLU(True),]\n",
        "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model1+=[nn.ReLU(True),]\n",
        "        model1+=[norm_layer(64),]\n",
        "        # add a subsampling operation\n",
        "\n",
        "        # Conv2\n",
        "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model2+=[nn.ReLU(True),]\n",
        "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model2+=[nn.ReLU(True),]\n",
        "        model2+=[norm_layer(128),]\n",
        "        # add a subsampling layer operation\n",
        "\n",
        "        # Conv3\n",
        "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[norm_layer(256),]\n",
        "        # add a subsampling layer operation\n",
        "\n",
        "        # Conv4\n",
        "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[norm_layer(512),]\n",
        "\n",
        "        # Conv5\n",
        "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[norm_layer(512),]\n",
        "\n",
        "        # Conv6\n",
        "        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[norm_layer(512),]\n",
        "\n",
        "        # Conv7\n",
        "        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[norm_layer(512),]\n",
        "\n",
        "        # Conv7\n",
        "        model8up=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True)]\n",
        "        model3short8=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "\n",
        "        model8=[nn.ReLU(True),]\n",
        "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model8+=[nn.ReLU(True),]\n",
        "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model8+=[nn.ReLU(True),]\n",
        "        model8+=[norm_layer(256),]\n",
        "\n",
        "        # Conv9\n",
        "        model9up=[nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=True),]\n",
        "        model2short9=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        # add the two feature maps above\n",
        "\n",
        "        model9=[nn.ReLU(True),]\n",
        "        model9+=[nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model9+=[nn.ReLU(True),]\n",
        "        model9+=[norm_layer(128),]\n",
        "\n",
        "        # Conv10\n",
        "        model10up=[nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=True),]\n",
        "        model1short10=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        # add the two feature maps above\n",
        "\n",
        "        model10=[nn.ReLU(True),]\n",
        "        model10+=[nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=True),]\n",
        "        model10+=[nn.LeakyReLU(negative_slope=.2),]\n",
        "\n",
        "        # classification output\n",
        "        model_class=[nn.Conv2d(256, classes, kernel_size=1, padding=0, dilation=1, stride=1, bias=True),]\n",
        "\n",
        "        # regression output\n",
        "        model_out=[nn.Conv2d(128, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=True),]\n",
        "        model_out+=[nn.Tanh()]\n",
        "\n",
        "        self.model1 = nn.Sequential(*model1)\n",
        "        self.model2 = nn.Sequential(*model2)\n",
        "        self.model3 = nn.Sequential(*model3)\n",
        "        self.model4 = nn.Sequential(*model4)\n",
        "        self.model5 = nn.Sequential(*model5)\n",
        "        self.model6 = nn.Sequential(*model6)\n",
        "        self.model7 = nn.Sequential(*model7)\n",
        "        self.model8up = nn.Sequential(*model8up)\n",
        "        self.model8 = nn.Sequential(*model8)\n",
        "        self.model9up = nn.Sequential(*model9up)\n",
        "        self.model9 = nn.Sequential(*model9)\n",
        "        self.model10up = nn.Sequential(*model10up)\n",
        "        self.model10 = nn.Sequential(*model10)\n",
        "        self.model3short8 = nn.Sequential(*model3short8)\n",
        "        self.model2short9 = nn.Sequential(*model2short9)\n",
        "        self.model1short10 = nn.Sequential(*model1short10)\n",
        "\n",
        "        self.model_class = nn.Sequential(*model_class)\n",
        "        self.model_out = nn.Sequential(*model_out)\n",
        "\n",
        "        self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='bilinear'),])\n",
        "        self.softmax = nn.Sequential(*[nn.Softmax(dim=1),])\n",
        "\n",
        "    def forward(self, input_A, input_B=None, mask_B=None):\n",
        "        if(input_B is None):\n",
        "            input_B = torch.cat((input_A*0, input_A*0), dim=1)\n",
        "        if(mask_B is None):\n",
        "            mask_B = input_A*0\n",
        "\n",
        "        conv1_2 = self.model1(torch.cat((self.normalize_l(input_A),self.normalize_ab(input_B),mask_B),dim=1))\n",
        "        conv2_2 = self.model2(conv1_2[:,:,::2,::2])\n",
        "        conv3_3 = self.model3(conv2_2[:,:,::2,::2])\n",
        "        conv4_3 = self.model4(conv3_3[:,:,::2,::2])\n",
        "        conv5_3 = self.model5(conv4_3)\n",
        "        conv6_3 = self.model6(conv5_3)\n",
        "        conv7_3 = self.model7(conv6_3)\n",
        "\n",
        "        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3)\n",
        "        conv8_3 = self.model8(conv8_up)\n",
        "        conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n",
        "        conv9_3 = self.model9(conv9_up)\n",
        "        conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n",
        "        conv10_2 = self.model10(conv10_up)\n",
        "        out_reg = self.model_out(conv10_2)\n",
        "\n",
        "        conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n",
        "        conv9_3 = self.model9(conv9_up)\n",
        "        conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n",
        "        conv10_2 = self.model10(conv10_up)\n",
        "        out_reg = self.model_out(conv10_2)\n",
        "\n",
        "        return self.unnormalize_ab(out_reg)\n",
        "\n",
        "def siggraph17(pretrained=True):\n",
        "    model = SIGGRAPHGenerator()\n",
        "    if(pretrained):\n",
        "        import torch.utils.model_zoo as model_zoo\n",
        "        model.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/siggraph17-df00044c.pth',map_location='cpu',check_hash=True))\n",
        "    return model\n",
        "# colorizer outputs 256x256 ab map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_video = '/content/sample_data/test2.mp4'\n",
        "\n",
        "\n",
        "# Load the input video\n",
        "clip = VideoFileClip(input_video)\n",
        "video_duration = clip.duration\n",
        "vd = math.ceil(video_duration)\n",
        "height = clip.size[0]\n",
        "width = clip.size[1]\n",
        "cpt = 0\n",
        "for it in range(0,vd,2):\n",
        "    start_time =it    # Start time in seconds\n",
        "    end_time =it+2   # End time in seconds\n",
        "\n",
        "    print(start_time)\n",
        "    print(end_time)\n",
        "    output_video = f'/content/sample_data/TARGET/output{cpt}.mp4'\n",
        "\n",
        "    # Crop the desired duration\n",
        "    cropped_clip = clip.subclip(start_time, end_time)\n",
        "\n",
        "    # Convert the cropped clip to grayscale\n",
        "    grayscale_clip = cropped_clip.fx(vfx.blackwhite)\n",
        "    grayscale_clip.ipython_display()\n",
        "    # Write the grayscale clip to a new video file\n",
        "    grayscale_clip.write_videofile(output_video, codec='libx264', audio_codec='aac', fps=30 )\n",
        "\n",
        "    # Spécifiez le chemin vers le fichier h5 du modèle\n",
        "    chemin_fichier = '/content/drive/MyDrive/VGG16_Grayscale (2).hdf5'\n",
        "\n",
        "    model = tf.keras.models.load_model(chemin_fichier)\n",
        "\n",
        "    frames = []\n",
        "    framesf = []\n",
        "    # Iterate over the frames and append them to the list\n",
        "    for frame in grayscale_clip.iter_frames():\n",
        "        plt.imshow(frame,cmap='gray')\n",
        "        framef = cv2.resize(frame, (256, 256))\n",
        "        framef = cv2.cvtColor(framef, cv2.COLOR_BGR2RGB)\n",
        "        framef = cv2.cvtColor(framef, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Calculate the cropping coordinates\n",
        "\n",
        "        framesf.append(framef)\n",
        "        frames.append(frame)\n",
        "\n",
        "\n",
        "\n",
        "    framesf = np.array(framesf)\n",
        "    # Preprocess the frames for prediction\n",
        "    framesf = framesf / 255.0\n",
        "    framesf = np.expand_dims(framesf, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    features=model.predict(framesf)\n",
        "\n",
        "    #SET THE K VALUE\n",
        "    k = 0\n",
        "    pairwise_distances = 0\n",
        "\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        pairwise_distances = distance.euclidean(features[i], features[i-1])\n",
        "        if(pairwise_distances>0.5):\n",
        "            k+=1\n",
        "            print(pairwise_distances)\n",
        "    if (k == 0):\n",
        "        k+=1\n",
        "\n",
        "    #THE KMEANS ALGORITHM\n",
        "\n",
        "    # Assuming 'features' is the 2D-array with shape (n_steps, feature-dimension)\n",
        "    X = features\n",
        "\n",
        "    # Set the number of clusters (K)\n",
        "    num_clusters = k\n",
        "\n",
        "    # Perform K-means clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "\n",
        "    keyframes = []\n",
        "\n",
        "    for center in cluster_centers:\n",
        "        distances = np.linalg.norm(X - center, axis=1)\n",
        "        closest_frame_index = np.argmin(distances)\n",
        "        keyframes.append(closest_frame_index)\n",
        "\n",
        "    # 'keyframes' now contains the indices of the keyframes in the original dataset\n",
        "\n",
        "    original_video_keyframes = [frames[keyframe_index] for keyframe_index in keyframes]\n",
        "\n",
        "\n",
        "\n",
        "    # Reshape the image data to (height, width, channels)\n",
        "\n",
        "    # Display the Centers of the clusters \"KEYFRAMES\"\n",
        "    plt.imshow(original_video_keyframes[0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.savefig('source.png')\n",
        "\n",
        "    #COLORIZE THE KEYFRAME\n",
        "    colorizer_eccv16 = eccv16(pretrained=True).eval()\n",
        "    #colorizer_siggraph17 = siggraph17(pretrained=True).eval()\n",
        "\n",
        "    img = original_video_keyframes[0]\n",
        "    (tens_l_orig, tens_l_rs) = preprocess_img(img, HW=(256,256))\n",
        "\n",
        "\n",
        "    img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n",
        "    #out_img_siggraph17 = postprocess_tens(tens_l_orig, colorizer_siggraph17(tens_l_rs).cpu())\n",
        "\n",
        "    out_img_eccv16 = postprocess_tens(tens_l_orig, colorizer_eccv16(tens_l_rs).cpu())\n",
        "\n",
        "\n",
        "    plt.imshow(out_img_eccv16)\n",
        "    print(out_img_eccv16.shape)\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f'/content/sample_data/REFS/source{cpt}.jpg',bbox_inches='tight', pad_inches=0)\n",
        "    cpt+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd  /content/drive/MyDrive/\n",
        "#!git clone https://github.com/zhangmozhe/Deep-Exemplar-based-Video-Colorization.git"
      ],
      "metadata": {
        "id": "WcSYZika8JCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEigjADV5Z8X"
      },
      "outputs": [],
      "source": [
        "%cd  /content/drive/MyDrive/Deep-Exemplar-based-Video-Colorization/\n",
        "#!git clone https://github.com/zhangmozhe/Deep-Exemplar-based-Video-Colorization.git\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu2Kmh_Z6RTh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGr-b3T06URc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G6dQckk9YDq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "import cv2\n",
        "from IPython.display import Image\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "basepath = \"my_test\"\n",
        "upload_path = os.path.join(basepath, \"upload\")\n",
        "upload_ref_path = os.path.join(basepath, \"upload_ref\")\n",
        "upload_output_path = os.path.join(basepath, \"upload_output\")\n",
        "\n",
        "if os.path.isdir(upload_ref_path):\n",
        "    shutil.rmtree(upload_ref_path)\n",
        "\n",
        "if os.path.isdir(upload_path):\n",
        "    shutil.rmtree(upload_path)\n",
        "\n",
        "if os.path.isdir(upload_output_path):\n",
        "    shutil.rmtree(upload_output_path)\n",
        "\n",
        "os.makedirs(upload_ref_path)\n",
        "os.makedirs(upload_path)\n",
        "os.makedirs(upload_output_path)\n",
        "for numvid in range(1,int(vd/2)):\n",
        "  print(\"Uploading the input video...\")\n",
        "  video_path = f\"/content/sample_data/TARGET/output{numvid}.mp4\"  # Replace with your video path\n",
        "\n",
        "  # Move the video file to the upload path\n",
        "  shutil.move(video_path, os.path.join(upload_path, \"input.mp4\"))\n",
        "\n",
        "  print(\"Video uploaded successfully.\")\n",
        "\n",
        "  # Define the path to the video file\n",
        "  path = os.path.join(upload_path, \"input.mp4\")\n",
        "\n",
        "  # Load the video clip\n",
        "  clip = VideoFileClip(path)\n",
        "\n",
        "  # Display the video clip\n",
        "  clip.ipython_display(width=600)\n",
        "\n",
        "\n",
        "\n",
        "  # Define the path to the reference image file\n",
        "  image_path = f\"/content/sample_data/REFS/source{numvid}.jpg\"  # Replace with the actual image path\n",
        "\n",
        "  # Set the filename for the reference image\n",
        "  filename = \"ref.jpg\"\n",
        "\n",
        "  # Move the reference image to the upload reference directory\n",
        "  shutil.move(image_path, os.path.join(upload_ref_path, filename))\n",
        "\n",
        "  # Display the reference image\n",
        "  Image(os.path.join(upload_ref_path, filename), width=400)\n",
        "\n",
        "\n",
        "\n",
        "  def FrameCapture(video_path, output_path):\n",
        "      vidObj = cv2.VideoCapture(video_path)\n",
        "      count = 0\n",
        "      success = 1\n",
        "      while success:\n",
        "          success, image = vidObj.read()\n",
        "          if success:\n",
        "              cv2.imwrite(output_path + \"/%d.jpg\" % count, image)\n",
        "              count += 1\n",
        "\n",
        "  path = upload_path\n",
        "  output_path = os.path.join(basepath, \"frames\")\n",
        "\n",
        "  videos = [video for video in os.listdir(path) if video.endswith(\".avi\") or video.endswith(\".mp4\")]\n",
        "  videos.sort()\n",
        "  print(videos)\n",
        "\n",
        "  print(\"extracting frames from input video\")\n",
        "  for idx, video in enumerate(videos):\n",
        "      frame_path = os.path.join(output_path, video.split(\".\")[0])\n",
        "      if not os.path.exists(frame_path):\n",
        "          os.makedirs(frame_path)\n",
        "      FrameCapture(os.path.join(path, video), frame_path)\n",
        "  # Define the path to the image file\n",
        "\n",
        "  image_path = './my_test/upload_ref/ref.jpg'\n",
        "\n",
        "  # Read the image from the specified path\n",
        "  image = cv2.imread(image_path)\n",
        "  print(image.shape)\n",
        "\n",
        "\n",
        "  !python test.py --clip_path ./my_test/frames/input \\\n",
        "               --ref_path ./my_test/upload_ref \\\n",
        "               --output_path ./my_test/upload_output\n",
        "\n",
        "  path= os.path.join(basepath, \"upload_output/input_ref/video.avi\")\n",
        "  clip=VideoFileClip(path)\n",
        "  clip.ipython_display(width=600)\n",
        "  import os\n",
        "\n",
        "  old_path = os.path.join(basepath, \"upload_output/input_ref/video.avi\")\n",
        "  new_path = os.path.join(basepath, f\"upload_output/input_ref/video{numvid}.avi\")\n",
        "\n",
        "  os.rename(old_path, new_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfgYxA7O-pS_"
      },
      "outputs": [],
      "source": [
        "\"\"\"import os\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "\n",
        "def concatenate_videos(folder_path, output_path):\n",
        "    video_clips = []\n",
        "\n",
        "    # Get a list of video files in the folder\n",
        "    video_files = sorted([f for f in os.listdir(folder_path) if f.startswith('video') and f.endswith('.mp4')])\n",
        "\n",
        "    # Load each video clip and add it to the list\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(folder_path, video_file)\n",
        "        clip = VideoFileClip(video_path)\n",
        "        video_clips.append(clip)\n",
        "\n",
        "    # Concatenate the video clips\n",
        "    final_clip = concatenate_videoclips(video_clips)\n",
        "\n",
        "    # Write the final concatenated video to the output path\n",
        "    final_clip.write_videofile(output_path, codec='libx264')\n",
        "\n",
        "    # Close the video clips\n",
        "    final_clip.close()\n",
        "    for clip in video_clips:\n",
        "        clip.close()\n",
        "\n",
        "# Example usage\n",
        "folder_path = '/content/sample_data/TARGET'  # Replace with the actual folder path\n",
        "output_path = '/content/sample_data/TARGET/outputbw.mp4'  # Replace with the desired output file path\n",
        "\n",
        "concatenate_videos(folder_path, output_path)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC2JD-i-8DM5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = \"/content/drive/MyDrive/Deep-Exemplar-based-Video-Colorization/my_test/upload_output/input_ref\"\n",
        "destination_folder = \"/content/sample_data/Results\"\n",
        "\n",
        "# Iterate over all files in the source folder\n",
        "for filename in os.listdir(source_folder):\n",
        "    file_path = os.path.join(source_folder, filename)\n",
        "\n",
        "    # Check if the file is a video file\n",
        "    if os.path.isfile(file_path) and filename.lower().endswith(('.mp4', '.avi', '.mov')):\n",
        "        # Move the video file to the destination folder\n",
        "        shutil.move(file_path, os.path.join(destination_folder, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rDJkCuHEaAq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eghthjEb-tTI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "\n",
        "def concatenate_videos(folder_path, output_path):\n",
        "    # Get a list of video files in the folder\n",
        "    video_files = [file for file in sorted(os.listdir(folder_path)) if file.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
        "\n",
        "    # Create an empty list to store the video clips\n",
        "    video_clips = []\n",
        "\n",
        "    # Iterate over the video files\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(folder_path, video_file)\n",
        "\n",
        "        # Load the video clip\n",
        "        clip = VideoFileClip(video_path)\n",
        "        video_clips.append(clip)\n",
        "\n",
        "    # Concatenate the video clips sequentially\n",
        "    final_clip = concatenate_videoclips(video_clips)\n",
        "    final_clip = final_clip.resize(height=height,width=width)\n",
        "\n",
        "    # Specify the output file path for the concatenated video\n",
        "    output_file = os.path.join(output_path, \"concatenated_video.avi\")\n",
        "\n",
        "    # Write the concatenated video to the output file\n",
        "    final_clip.write_videofile(output_file, codec=\"mpeg4\")\n",
        "\n",
        "    print(\"Videos have been concatenated.\")\n",
        "\n",
        "# Specify the folder containing the video files\n",
        "folder_path = \"/content/sample_data/Results\"\n",
        "\n",
        "# Specify the output path for the concatenated video\n",
        "output_path = \"/content/sample_data/Results\"\n",
        "\n",
        "# Call the function to concatenate the videos\n",
        "concatenate_videos(folder_path, output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4_iocmSCJOS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvzNy_OlCLqg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb6U6VMLCOVB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDylviVjCVcR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}